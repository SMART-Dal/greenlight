2023-12-09 03:45:50.611218: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2023-12-09 03:45:51.367062: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
[EXECUTION] [03:45:51] Waiting before running function for 10 seconds.
[EXECUTION] [03:46:01] Sensors started
[STABLE CHECK] [03:46:04] Temperature too high: mean is 43.45, which is greater than 40
[STABLE CHECK] [03:46:04] Temperature is too high.
[STABLE CHECK] [03:46:04] Waiting 20 seconds to reach low temperature.

[STABLE CHECK] [03:46:24] Temperature too high: mean is 40.5, which is greater than 40
[STABLE CHECK] [03:46:24] Temperature is too high.
[STABLE CHECK] [03:46:24] Waiting 20 seconds to reach low temperature.

[STABLE CHECK] [03:46:44] Success: temperature is below threshold.
[MAIN] [03:46:44] Terminated sensors
[STABLE CHECK] [03:46:44] Waiting 20 seconds to reach stable state.

[STABLE CHECK] [03:47:04] Success: Machine is stable.
[EXECUTION] [03:47:04] Successfully reached stable state
Epoch 1/5

1/5 [=====>........................] - ETA: 10s - loss: 2.6950 - accuracy: 0.1406
5/5 [==============================] - 3s 7ms/step - loss: 1.5789 - accuracy: 0.5063
2023-12-09 03:47:07.310474: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Epoch 2/5

1/5 [=====>........................] - ETA: 0s - loss: 0.5856 - accuracy: 0.8594
5/5 [==============================] - 0s 3ms/step - loss: 0.4764 - accuracy: 0.9062
2023-12-09 03:47:07.366592: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Epoch 3/5

1/5 [=====>........................] - ETA: 0s - loss: 0.3572 - accuracy: 0.9531
5/5 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.9625
2023-12-09 03:47:07.412806: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Epoch 4/5

1/5 [=====>........................] - ETA: 0s - loss: 0.2327 - accuracy: 1.0000
5/5 [==============================] - 0s 2ms/step - loss: 0.2367 - accuracy: 0.9719
2023-12-09 03:47:07.464536: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Epoch 5/5

1/5 [=====>........................] - ETA: 0s - loss: 0.1921 - accuracy: 1.0000
5/5 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.9875
2023-12-09 03:47:07.511500: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
[EXECUTION] [03:47:07] waiting idle for 30 seconds after function execution
[EXECUTION] [03:47:37] Performed tensorflow.keras.Sequential.fit() on input and will now save energy data.
inside_arg_size
++++++4++++++
++++++5++++++
5-size 197344.0
4-size 197344.0
++++++5++++++
5-size 197344.0
outside_arg_size
inside_kwarg_size
++++++4++++++
++++++5++++++
5-size 56.0
4-size 56.0
++++++5++++++
5-size 56.0
outside_kwarg_size
inside_object_size
++++++5++++++
++++++6++++++ this __dict__ descriptor does not support '_DictWrapper' objects
6-size 48.0
outside_object_size
[EXECUTION] [03:47:37] Result: {'tensorflow.keras.Sequential.fit()': {'energy_data': {'cpu': '{"columns":["time_elapsed","energy (J
[EXECUTION] [03:47:37] Data written to file /home/saurabh/method-energy-dataset/dataset/tensorflow_docs/effective_tf2/experiment-5.json

1/5 [=====>........................] - ETA: 0s - loss: 1.5465 - accuracy: 0.5781
5/5 [==============================] - 0s 2ms/step - loss: 1.5637 - accuracy: 0.6125
2023-12-09 03:47:37.859963: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:38.401910: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:38.463435: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:38.517168: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:38.571954: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:38.628969: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:39.360066: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:39.420595: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:39.478726: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:39.534605: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
2023-12-09 03:47:39.593804: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Loss 1.5636804103851318, Accuracy 0.612500011920929
Finished epoch 0
Finished epoch 1
Finished epoch 2
Finished epoch 3
Finished epoch 4
(10, 20, 13)
Epoch:  0
  loss:     0.176
  accuracy: 0.984
Epoch:  1
  loss:     0.154
  accuracy: 0.991
Epoch:  2
  loss:     0.137
  accuracy: 0.994
Epoch:  3
  loss:     0.121
  accuracy: 0.991
Epoch:  4
  loss:     0.105
  accuracy: 0.997
[EXECUTION] [03:47:39] Waiting before running function for 10 seconds.
[EXECUTION] [03:47:49] Sensors started
[STABLE CHECK] [03:47:52] Temperature too high: mean is 44.2, which is greater than 40
[STABLE CHECK] [03:47:52] Temperature is too high.
[STABLE CHECK] [03:47:52] Waiting 20 seconds to reach low temperature.

[STABLE CHECK] [03:48:12] Temperature too high: mean is 41, which is greater than 40
[STABLE CHECK] [03:48:12] Temperature is too high.
[STABLE CHECK] [03:48:12] Waiting 20 seconds to reach low temperature.

[STABLE CHECK] [03:48:32] Temperature too high: mean is 40.5, which is greater than 40
[STABLE CHECK] [03:48:32] Temperature is too high.
[STABLE CHECK] [03:48:32] Waiting 20 seconds to reach low temperature.

[STABLE CHECK] [03:48:52] Success: temperature is below threshold.
[MAIN] [03:48:52] Terminated sensors
[STABLE CHECK] [03:48:52] Waiting 20 seconds to reach stable state.

[STABLE CHECK] [03:49:12] Success: Machine is stable.
[EXECUTION] [03:49:12] Successfully reached stable state

1/5 [=====>........................] - ETA: 2s - loss: 0.0949 - acc: 1.0000 - accuracy: 1.0000 - my_accuracy: 1.0000
5/5 [==============================] - 1s 5ms/step - loss: 0.1211 - acc: 0.9906 - accuracy: 0.9906 - my_accuracy: 0.9906
2023-12-09 03:49:13.428174: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
[EXECUTION] [03:49:13] waiting idle for 30 seconds after function execution
[EXECUTION] [03:49:43] Performed tensorflow.keras.Sequential.fit() on input and will now save energy data.
inside_arg_size
++++++4++++++
++++++5++++++
5-size 197416.0
4-size 197416.0
++++++5++++++
5-size 197416.0
outside_arg_size
inside_kwarg_size
outside_kwarg_size
inside_object_size
++++++5++++++
++++++6++++++ this __dict__ descriptor does not support '_DictWrapper' objects
6-size 48.0
outside_object_size
[EXECUTION] [03:49:43] Result: {'tensorflow.keras.Sequential.fit()': {'energy_data': {'cpu': '{"columns":["time_elapsed","energy (J
[EXECUTION] [03:49:43] Data written to file /home/saurabh/method-energy-dataset/dataset/tensorflow_docs/effective_tf2/experiment-5.json
